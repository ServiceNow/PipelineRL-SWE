import logging
import re
from typing import Annotated, Any, Generator, Literal, TypeAlias, Union, List
from typing import Dict

from pydantic import Field

from tapeagents.agent import Agent
from tapeagents.core import (
    LLMOutputParsingFailureAction,
    Observation,
    Prompt,
    Step,
    Tape,
    Action,
    FinalStep,
)
from tapeagents.llms import LLM
from tapeagents.nodes import StandardNode

from pipelinerl.swe.rollouts.a2a import ExpertModelAdvice

logger = logging.getLogger(__name__)


class LocalizationTask(Observation):
    """Task step containing the problem statement for file localization."""
    kind: Literal["localization_task"] = "localization_task"
    problem_statement: str = Field(description="The issue description to localize relevant files for")
    file_stats: Dict[str, Dict] = Field(default_factory=dict) # file stats
    
    def llm_view(self, indent: int | None = 2) -> str:
        return f"Issue to localize: {self.problem_statement}"


class LocalizationQuery(Action):
    """Action containing multiple search queries generated by the model for BM25 search."""
    kind: Literal["localization_query"] = "localization_query"
    queries: List[str] = Field(description="List of search queries for BM25")
    reasoning: str = Field(default="", description="The reasoning process used to generate the queries")
    format_penalty: float = Field(default=0.0, description="Penalty for extra/garbage content in output")
    garbage_content: str = Field(default="", description="The garbage content that was found")

    @property
    def num_queries(self) -> int:
        """Return the number of queries."""
        return len(self.queries)


LocalizationStep: TypeAlias = Annotated[
    LocalizationQuery,
    Field(discriminator="kind"),
]

LocalizationTape = Tape[
    None,
    Union[
        LocalizationTask,
        LocalizationQuery,
        LLMOutputParsingFailureAction,
        ExpertModelAdvice,  # Add this
    ],
]


class LocalizationNode(StandardNode):
    """Node that generates search queries for file localization."""
    
    max_prompt_length: int = 8000  # Reasonable length for problem statements
    
    def parse_completion(self, completion: str) -> Generator[Step, None, None]:
        """Parse the LLM completion to extract multiple search queries."""
        try:
            # Find all query tags
            query_pattern = r'<query>(.*?)</query>'
            query_matches = re.findall(query_pattern, completion, re.DOTALL)
            
            if not query_matches:
                yield LLMOutputParsingFailureAction(
                    error="No <query> tags found", 
                    llm_output=completion
                )
                return
            
            if len(query_matches) > 10:
                yield LLMOutputParsingFailureAction(
                    error=f"Too many queries: {len(query_matches)} (max 10)", 
                    llm_output=completion
                )
                return
            
            # Extract and validate queries
            queries = []
            for query_text in query_matches:
                query = query_text.strip()
                if not query:
                    yield LLMOutputParsingFailureAction(
                        error="Empty query found in query tags", 
                        llm_output=completion
                    )
                    return
                queries.append(query)
            
            # Extract reasoning (everything before the first query tag)
            reasoning = ""
            if '<query>' in completion:
                reasoning = completion.split('<query>')[0].strip()
            
            # Check for garbage content after the last expected tag
            format_penalty = 0.0
            garbage_content = ""
            
            # Find the position of the last query tag
            last_query_matches = list(re.finditer(r'</query>', completion))
            
            if last_query_matches:
                last_tag_pos = last_query_matches[-1].end()
                
                # Check what comes after the last expected tag
                content_after = completion[last_tag_pos:].strip()
                
                if content_after and content_after != '<|im_end|>':
                    garbage_content = content_after
                    format_penalty = 0.5  # Apply -0.5 penalty for garbage content
                    
                    logger.info(f"Garbage content detected after last tag: {repr(content_after[:100])}")
            
            yield LocalizationQuery(
                queries=queries, 
                reasoning=reasoning,
                format_penalty=format_penalty,
                garbage_content=garbage_content
            )
            
        except Exception as e:
            logger.info(f"Failed to parse localization query: {completion}\n\nError: {e}")
            yield LLMOutputParsingFailureAction(
                error=f"Failed to parse localization query: {e}", 
                llm_output=completion
            )

    def make_prompt(self, agent: Any, tape: Tape) -> Prompt:
        """Create a prompt for the model to generate search queries."""
        # Extract expert feedback and task from tape
        expert_feedback = None
        task = None
        
        for step in tape.steps:
            if hasattr(step, 'kind') and step.kind == "expert_model_advice":
                expert_feedback = step
            elif isinstance(step, LocalizationTask):
                task = step
        
        assert task is not None, f"No LocalizationTask found in tape steps: {[type(s).__name__ for s in tape.steps]}"
        
        # Extract unique directories from file paths
        directories = set()
        for file_path in task.file_stats.keys():
            if '/' in file_path:
                dir_name = '/'.join(file_path.split('/')[:-1])
                directories.add(dir_name)
            else:
                directories.add('.')  # Root directory
        
        # Create simple directory listing
        dirs_text = "\n".join(sorted(directories))
        
        system_content = (
            "You are an expert software engineer tasked with generating BM25 search queries to find "
            "relevant files in a codebase for fixing a given issue.\n\n"
            "You will be shown the directory structure of the repository. Your goal is to create "
            "search queries that will rank the most relevant files as highly as possible using "
            "BM25 keyword matching.\n\n"
            "Generate 1-10 focused search queries. You can use multiple queries to cover different "
            "aspects of the issue or different terminology that might be used. The total retrieval "
            "budget is 10 files, which will be split among your queries, so more queries means "
            "fewer results per query.\n\n"
            "Focus on:\n"
            "- Keywords from the issue description\n"
            "- Technical terms, class names, method names\n"
            "- Domain-specific vocabulary\n"
            "- File extensions or patterns if relevant\n\n"
            "IMPORTANT: Your response must end immediately after the last </query> tag. "
            "Any additional content after the final query tag will result in a penalty.\n\n"
            "Format your response as:\n"
            "[Your reasoning and analysis]\n\n"
            "<query>first search query</query>\n"
            "<query>second search query</query>\n"
            "... (up to 10 queries total)"
        )
        
        user_content = (
            f"Repository directories:\n{dirs_text}\n\n"
            f"Issue to analyze and create BM25 search queries for:\n\n"
            f"{task.problem_statement}\n\n"
        )
        
        # Add expert feedback if present
        if expert_feedback:
            user_content = expert_feedback.llm_view() + "\n\n" + user_content
        
        user_content += (
            f"Please analyze this issue and create optimized BM25 search queries. "
            f"Remember to end your response immediately after the last query tag."
        )
        
        system_message = {
            "role": "system",
            "content": system_content
        }
        
        user_message = {
            "role": "user", 
            "content": user_content
        }
        
        messages = [system_message, user_message]
        
        # Apply token limit if we have a tokenizer
        prompt_token_ids = None
        if hasattr(agent, 'llm') and hasattr(agent.llm, 'tokenizer') and agent.llm.tokenizer:
            prompt_token_ids = agent.llm.tokenizer.apply_chat_template(
                messages, add_special_tokens=True, add_generation_prompt=True
            )
            prompt_token_ids = prompt_token_ids[-self.max_prompt_length:]
            
        return Prompt(messages=messages, token_ids=prompt_token_ids)


class LocalizationAgent(Agent):
    """Simple one-step agent for generating file localization queries."""
    
    @classmethod
    def create(cls, system_prompt: str = None, llm: LLM = None, max_prompt_length: int = 8000):
        """Create a LocalizationAgent.
        
        Args:
            system_prompt: Optional system prompt override
            llm: The LLM to use
            max_prompt_length: Maximum prompt length in tokens
        """
        # Handle the llm parameter correctly for the Agent base class
        llms = llm
        if llm is not None and not isinstance(llm, dict):
            llms = {"default": llm}
            
        agent = super().create(
            llms=llms,
            nodes=[
                LocalizationNode(
                    name="localization",
                    agent_step_cls=LocalizationStep,
                    system_prompt=system_prompt if system_prompt else "",
                    max_prompt_length=max_prompt_length,
                ),
            ],
            max_iterations=1,  # Single step agent
        )
        agent.store_llm_calls = True
        if llm:
            agent.llm.load_tokenizer()
        return agent