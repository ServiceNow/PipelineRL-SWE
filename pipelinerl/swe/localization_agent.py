import logging
import re
from typing import Annotated, Any, Generator, Literal, TypeAlias, Union, List
from typing import Dict

from pydantic import Field

from tapeagents.agent import Agent
from tapeagents.core import (
    LLMOutputParsingFailureAction,
    Observation,
    Prompt,
    Step,
    Tape,
    Action,
    FinalStep,
)
from tapeagents.llms import LLM
from tapeagents.nodes import StandardNode

logger = logging.getLogger(__name__)


class LocalizationTask(Observation):
    """Task step containing the problem statement for file localization."""
    kind: Literal["localization_task"] = "localization_task"
    problem_statement: str = Field(description="The issue description to localize relevant files for")
    file_stats: Dict[str, Dict] = Field(default_factory=dict) # file stats
    
    def llm_view(self, indent: int | None = 2) -> str:
        return f"Issue to localize: {self.problem_statement}"


class LocalizationQuery(Action):
    """Action containing multiple search queries generated by the model for BM25 search."""
    kind: Literal["localization_query"] = "localization_query"
    num_queries: int = Field(description="Number of queries to generate (1-3)")
    queries: List[str] = Field(description="List of search queries for BM25")
    reasoning: str = Field(default="", description="The reasoning process used to generate the queries")


LocalizationStep: TypeAlias = Annotated[
    LocalizationQuery,
    Field(discriminator="kind"),
]

LocalizationTape = Tape[
    None,
    Union[
        LocalizationTask,
        LocalizationQuery,
        LLMOutputParsingFailureAction,
    ],
]


class LocalizationNode(StandardNode):
    """Node that generates search queries for file localization."""
    
    max_prompt_length: int = 8000  # Reasonable length for problem statements
    
    def parse_completion(self, completion: str) -> Generator[Step, None, None]:
        """Parse the LLM completion to extract multiple search queries."""
        try:
            # Look for num_queries
            num_match = re.search(r'<num_queries>(\d+)</num_queries>', completion)
            if not num_match:
                yield LLMOutputParsingFailureAction(
                    error="Missing <num_queries> tags", 
                    llm_output=completion
                )
                return
                
            num_queries = int(num_match.group(1))
            if num_queries < 1 or num_queries > 3:
                yield LLMOutputParsingFailureAction(
                    error=f"Invalid num_queries: {num_queries} (must be 1-3)", 
                    llm_output=completion
                )
                return
            
            # Extract queries
            queries = []
            for i in range(1, num_queries + 1):
                query_pattern = f'<query_{i}>(.*?)</query_{i}>'
                query_match = re.search(query_pattern, completion, re.DOTALL)
                if not query_match:
                    yield LLMOutputParsingFailureAction(
                        error=f"Missing <query_{i}> tags", 
                        llm_output=completion
                    )
                    return
                
                query = query_match.group(1).strip()
                if not query:
                    yield LLMOutputParsingFailureAction(
                        error=f"Empty query found in query_{i} tags", 
                        llm_output=completion
                    )
                    return
                
                queries.append(query)
            
            # Extract reasoning (everything before the num_queries tags)
            reasoning = ""
            if '<num_queries>' in completion:
                reasoning = completion.split('<num_queries>')[0].strip()
            
            yield LocalizationQuery(num_queries=num_queries, queries=queries, reasoning=reasoning)
            
        except ValueError as e:
            yield LLMOutputParsingFailureAction(
                error=f"Invalid number format in num_queries: {e}", 
                llm_output=completion
            )
        except Exception as e:
            logger.info(f"Failed to parse localization query: {completion}\n\nError: {e}")
            yield LLMOutputParsingFailureAction(
                error=f"Failed to parse localization query: {e}", 
                llm_output=completion
            )

    def make_prompt(self, agent: Any, tape: Tape) -> Prompt:
        """Create a prompt for the model to generate search queries."""
        # The tape should contain just the localization task
        task = tape.steps[0]
        assert isinstance(task, LocalizationTask), f"Expected LocalizationTask, got {task.__class__.__name__}"
        file_list = list(task.file_stats.keys())
        
        # Group files by directory to compress representation
        # Create interleaved representation: structure + inline key terms
        dir_groups = {}

        for file_path in sorted(task.file_stats.keys()):
            if '/' in file_path:
                dir_name = '/'.join(file_path.split('/')[:-1])
                file_name = file_path.split('/')[-1]
            else:
                dir_name = '.'
                file_name = file_path
            
            if dir_name not in dir_groups:
                dir_groups[dir_name] = []
            
            # Get key terms for this file
            stats = task.file_stats[file_path]
            top_terms = stats.get('top_terms', [])
            
            # Create file entry with optional terms
            if top_terms:
                terms_str = ', '.join(top_terms[:3])  # Limit to 3 terms to save tokens
                file_entry = f"{file_name} ({terms_str})"
            else:
                file_entry = file_name
            
            dir_groups[dir_name].append(file_entry)

        # Create final representation
        files_text_parts = []
        for dir_name, files in sorted(dir_groups.items()):
            if dir_name == '.':
                files_text_parts.append(f"Root: {'; '.join(sorted(files))}")
            else:
                files_text_parts.append(f"{dir_name}/: {'; '.join(sorted(files))}")

        files_text = "\n".join(files_text_parts)
        
        system_message = {
            "role": "system",
            "content": (
                "You are an expert software engineer tasked with generating BM25 search queries to find "
                "relevant files in a codebase for fixing a given issue.\n\n"
                "You will be shown each file in the repository along with its most important/distinctive "
                "terms (computed using TF-IDF). These terms represent the key concepts, classes, methods, "
                "and domain-specific vocabulary in each file.\n\n"
                "Your goal is to create search queries that will rank the most relevant files as highly "
                "as possible using BM25 keyword matching. Focus on terms that appear in the issue "
                "description and match the key terms of relevant files.\n\n"
                "You can generate 1-3 queries:\n"
                "- Use 1 query for simple, focused issues\n"
                "- Use 2-3 queries for complex issues that might need files with different terminology\n"
                "- Each additional query has a small cost, so only use multiple queries when beneficial\n\n"
                "The total retrieval budget is 10 files, split among your queries.\n"
                "Generate focused BM25 search queries designed to rank the gold standard files "
                "as highly as possible.\n\n"
                "Format your response as:\n"
                "<thinking>[Your reasoning and analysis]</thinking>\n\n"
                "<num_queries>1, 2, or 3</num_queries>\n\n"
                "<query_1>first search query</query_1>\n"
                "<query_2>second search query (if num_queries > 1)</query_2>\n"
                "<query_3>third search query (if num_queries > 2)</query_3>"
            )
        }
        
        user_message = {
            "role": "user", 
            "content": (
                f"Repository structure with key terms:\n{files_text}\n\n"
                f"Issue to analyze and create BM25 search queries for:\n\n"
                f"{task.problem_statement}\n\n"
                f"Please analyze this issue and the key terms shown for each file. Create optimized BM25 search queries "
                f"that include terms from the issue description that match the key terms of the most relevant files. "
                f"Focus on domain-specific identifiers, class names, and technical concepts."
            )
        }
        
        messages = [system_message, user_message]
        
        # Apply token limit if we have a tokenizer
        prompt_token_ids = None
        if hasattr(agent, 'llm') and hasattr(agent.llm, 'tokenizer') and agent.llm.tokenizer:
            prompt_token_ids = agent.llm.tokenizer.apply_chat_template(
                messages, add_special_tokens=True, add_generation_prompt=True
            )
            prompt_token_ids = prompt_token_ids[-self.max_prompt_length:]
            
        return Prompt(messages=messages, token_ids=prompt_token_ids)


class LocalizationAgent(Agent):
    """Simple one-step agent for generating file localization queries."""
    
    @classmethod
    def create(cls, system_prompt: str = None, llm: LLM = None, max_prompt_length: int = 8000):
        """Create a LocalizationAgent.
        
        Args:
            system_prompt: Optional system prompt override
            llm: The LLM to use
            max_prompt_length: Maximum prompt length in tokens
        """
        # Handle the llm parameter correctly for the Agent base class
        llms = llm
        if llm is not None and not isinstance(llm, dict):
            llms = {"default": llm}
            
        agent = super().create(
            llms=llms,
            nodes=[
                LocalizationNode(
                    name="localization",
                    agent_step_cls=LocalizationStep,
                    system_prompt=system_prompt if system_prompt else "",
                    max_prompt_length=max_prompt_length,
                ),
            ],
            max_iterations=1,  # Single step agent
        )
        agent.store_llm_calls = True
        if llm:
            agent.llm.load_tokenizer()
        return agent