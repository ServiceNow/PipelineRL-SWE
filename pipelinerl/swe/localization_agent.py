import logging
import re
from typing import Annotated, Any, Generator, Literal, TypeAlias, Union, List
from typing import Dict

from pydantic import Field

from tapeagents.agent import Agent
from tapeagents.core import (
    LLMOutputParsingFailureAction,
    Observation,
    Prompt,
    Step,
    Tape,
    Action,
    FinalStep,
)
from tapeagents.llms import LLM
from tapeagents.nodes import StandardNode

logger = logging.getLogger(__name__)


class LocalizationTask(Observation):
    """Task step containing the problem statement for file localization."""
    kind: Literal["localization_task"] = "localization_task"
    problem_statement: str = Field(description="The issue description to localize relevant files for")
    file_stats: Dict[str, Dict] = Field(default_factory=dict) # file stats
    
    def llm_view(self, indent: int | None = 2) -> str:
        return f"Issue to localize: {self.problem_statement}"


class LocalizationQuery(Action):
    """Action containing multiple search queries generated by the model for BM25 search."""
    kind: Literal["localization_query"] = "localization_query"
    num_queries: int = Field(description="Number of queries to generate (1-3)")
    queries: List[str] = Field(description="List of search queries for BM25")
    reasoning: str = Field(default="", description="The reasoning process used to generate the queries")


LocalizationStep: TypeAlias = Annotated[
    LocalizationQuery,
    Field(discriminator="kind"),
]

LocalizationTape = Tape[
    None,
    Union[
        LocalizationTask,
        LocalizationQuery,
        LLMOutputParsingFailureAction,
    ],
]


class LocalizationNode(StandardNode):
    """Node that generates search queries for file localization."""
    
    max_prompt_length: int = 8000  # Reasonable length for problem statements
    
    def parse_completion(self, completion: str) -> Generator[Step, None, None]:
        """Parse the LLM completion to extract multiple search queries."""
        try:
            # Look for num_queries
            num_match = re.search(r'<num_queries>(\d+)</num_queries>', completion)
            if not num_match:
                yield LLMOutputParsingFailureAction(
                    error="Missing <num_queries> tags", 
                    llm_output=completion
                )
                return
                
            num_queries = int(num_match.group(1))
            if num_queries < 1 or num_queries > 3:
                yield LLMOutputParsingFailureAction(
                    error=f"Invalid num_queries: {num_queries} (must be 1-3)", 
                    llm_output=completion
                )
                return
            
            # Extract queries
            queries = []
            for i in range(1, num_queries + 1):
                query_pattern = f'<query_{i}>(.*?)</query_{i}>'
                query_match = re.search(query_pattern, completion, re.DOTALL)
                if not query_match:
                    yield LLMOutputParsingFailureAction(
                        error=f"Missing <query_{i}> tags", 
                        llm_output=completion
                    )
                    return
                
                query = query_match.group(1).strip()
                if not query:
                    yield LLMOutputParsingFailureAction(
                        error=f"Empty query found in query_{i} tags", 
                        llm_output=completion
                    )
                    return
                
                queries.append(query)
            
            # Extract reasoning (everything before the num_queries tags)
            reasoning = ""
            if '<num_queries>' in completion:
                reasoning = completion.split('<num_queries>')[0].strip()
            
            yield LocalizationQuery(num_queries=num_queries, queries=queries, reasoning=reasoning)
            
        except ValueError as e:
            yield LLMOutputParsingFailureAction(
                error=f"Invalid number format in num_queries: {e}", 
                llm_output=completion
            )
        except Exception as e:
            logger.info(f"Failed to parse localization query: {completion}\n\nError: {e}")
            yield LLMOutputParsingFailureAction(
                error=f"Failed to parse localization query: {e}", 
                llm_output=completion
            )

    def make_prompt(self, agent: Any, tape: Tape) -> Prompt:
        """Create a prompt for the model to generate search queries."""
        # The tape should contain just the localization task
        task = tape.steps[0]
        assert isinstance(task, LocalizationTask), f"Expected LocalizationTask, got {task.__class__.__name__}"
        file_list = list(task.file_stats.keys())
        
        # Group files by directory to compress representation
        dir_groups = {}
        for file_path in file_list:
            if '/' in file_path:
                dir_name = '/'.join(file_path.split('/')[:-1])
                file_name = file_path.split('/')[-1]
            else:
                dir_name = '.'  # root directory
                file_name = file_path
            
            if dir_name not in dir_groups:
                dir_groups[dir_name] = []
            dir_groups[dir_name].append(file_name)
        
        # Create compressed representation
        files_text_parts = []
        for dir_name, files in sorted(dir_groups.items()):
            if dir_name == '.':
                files_text_parts.append(f"Root: {', '.join(sorted(files))}")
            else:
                files_text_parts.append(f"{dir_name}/: {', '.join(sorted(files))}")
        
        files_text = "\n".join(files_text_parts)
        
        system_message = {
            "role": "system",
            "content": (
                "You are an expert software engineer tasked with generating BM25 search queries to find "
                "relevant files in a codebase for fixing a given issue.\n\n"
                "Your goal is to create search queries that will rank the most relevant files as highly "
                "as possible using BM25 keyword matching. BM25 works by matching keywords.\n\n"
                "You can generate 1-3 queries:\n"
                "- Use 1 query for simple, focused issues\n"
                "- Use 2-3 queries for complex issues that might need files with different terminology\n"
                "- Each additional query has a small cost, so only use multiple queries when beneficial\n\n"
                "The total retrieval budget is 100 files, split among your queries.\n"
                "Generate focused BM25 search queries designed to rank the gold standard files "
                "as highly as possible.\n\n"
                "Format your response as:\n"
                "<thinking>[Your reasoning and analysis]</thinking>\n\n"
                "<num_queries>1, 2, or 3</num_queries>\n\n"
                "<query_1>first search query</query_1>\n"
                "<query_2>second search query (if num_queries > 1)</query_2>\n"
                "<query_3>third search query (if num_queries > 2)</query_3>"
            )
        }
        
        user_message = {
            "role": "user", 
            "content": (
                f"Repository structure (grouped by directory):\n{files_text}\n\n"
                f"Issue to analyze and create BM25 search queries for:\n\n"
                f"{task.problem_statement}\n\n"
                f"Please analyze this issue and provide optimized BM25 search queries to rank the most relevant files highly."
            )
        }
        
        messages = [system_message, user_message]
        
        # Apply token limit if we have a tokenizer
        prompt_token_ids = None
        if hasattr(agent, 'llm') and hasattr(agent.llm, 'tokenizer') and agent.llm.tokenizer:
            prompt_token_ids = agent.llm.tokenizer.apply_chat_template(
                messages, add_special_tokens=True, add_generation_prompt=True
            )
            prompt_token_ids = prompt_token_ids[-self.max_prompt_length:]
            
        return Prompt(messages=messages, token_ids=prompt_token_ids)


class LocalizationAgent(Agent):
    """Simple one-step agent for generating file localization queries."""
    
    @classmethod
    def create(cls, system_prompt: str = None, llm: LLM = None, max_prompt_length: int = 8000):
        """Create a LocalizationAgent.
        
        Args:
            system_prompt: Optional system prompt override
            llm: The LLM to use
            max_prompt_length: Maximum prompt length in tokens
        """
        # Handle the llm parameter correctly for the Agent base class
        llms = llm
        if llm is not None and not isinstance(llm, dict):
            llms = {"default": llm}
            
        agent = super().create(
            llms=llms,
            nodes=[
                LocalizationNode(
                    name="localization",
                    agent_step_cls=LocalizationStep,
                    system_prompt=system_prompt if system_prompt else "",
                    max_prompt_length=max_prompt_length,
                ),
            ],
            max_iterations=1,  # Single step agent
        )
        agent.store_llm_calls = True
        if llm:
            agent.llm.load_tokenizer()
        return agent