import logging
import re
from typing import Annotated, Any, Generator, Literal, TypeAlias, Union
from typing import Dict

from pydantic import Field

from tapeagents.agent import Agent
from tapeagents.core import (
    LLMOutputParsingFailureAction,
    Observation,
    Prompt,
    Step,
    Tape,
    Action,
    FinalStep,
)
from tapeagents.llms import LLM
from tapeagents.nodes import StandardNode

logger = logging.getLogger(__name__)


class LocalizationTask(Observation):
    """Task step containing the problem statement for file localization."""
    kind: Literal["localization_task"] = "localization_task"
    problem_statement: str = Field(description="The issue description to localize relevant files for")
    file_stats: Dict[str, Dict] = Field(default_factory=dict) # file stats
    
    def llm_view(self, indent: int | None = 2) -> str:
        return f"Issue to localize: {self.problem_statement}"


class LocalizationQuery(Action):
    """Action containing a search query generated by the model for BM25 search."""
    kind: Literal["localization_query"] = "localization_query"
    query: str = Field(description="Search query to find relevant files using BM25")
    reasoning: str = Field(default="", description="The reasoning process used to generate the query")


LocalizationStep: TypeAlias = Annotated[
    LocalizationQuery,
    Field(discriminator="kind"),
]

LocalizationTape = Tape[
    None,
    Union[
        LocalizationTask,
        LocalizationQuery,
        LLMOutputParsingFailureAction,
    ],
]


class LocalizationNode(StandardNode):
    """Node that generates search queries for file localization."""
    
    max_prompt_length: int = 8000  # Reasonable length for problem statements
    
    def parse_completion(self, completion: str) -> Generator[Step, None, None]:
        """Parse the LLM completion to extract the search query."""
        try:
            # Look for the query within <query></query> tags
            query_match = re.search(r'<query>(.*?)</query>', completion, re.DOTALL)
            
            if not query_match:
                yield LLMOutputParsingFailureAction(
                    error="Could not find search query in <query></query> tags", 
                    llm_output=completion
                )
                return
            
            query = query_match.group(1).strip()
            
            if not query:
                yield LLMOutputParsingFailureAction(
                    error="Empty search query found in tags", 
                    llm_output=completion
                )
                return
            
            # Extract reasoning (everything before the query tags)
            reasoning = ""
            if '<query>' in completion:
                reasoning = completion.split('<query>')[0].strip()
            
            yield LocalizationQuery(query=query, reasoning=reasoning)
            
        except Exception as e:
            logger.info(f"Failed to parse localization query: {completion}\n\nError: {e}")
            yield LLMOutputParsingFailureAction(
                error=f"Failed to parse localization query: {e}", 
                llm_output=completion
            )

    def make_prompt(self, agent: Any, tape: Tape) -> Prompt:
        """Create a prompt for the model to generate a search query."""
        # The tape should contain just the localization task
        task = tape.steps[0]
        assert isinstance(task, LocalizationTask), f"Expected LocalizationTask, got {task.__class__.__name__}"
        file_list = list(task.file_stats.keys())
        
        # Group files by directory to compress representation
        dir_groups = {}
        for file_path in file_list:
            if '/' in file_path:
                dir_name = '/'.join(file_path.split('/')[:-1])
                file_name = file_path.split('/')[-1]
            else:
                dir_name = '.'  # root directory
                file_name = file_path
            
            if dir_name not in dir_groups:
                dir_groups[dir_name] = []
            dir_groups[dir_name].append(file_name)
        
        # Create compressed representation
        files_text_parts = []
        for dir_name, files in sorted(dir_groups.items()):
            if dir_name == '.':
                files_text_parts.append(f"Root: {', '.join(sorted(files))}")
            else:
                files_text_parts.append(f"{dir_name}/: {', '.join(sorted(files))}")
        
        files_text = "\n".join(files_text_parts)
        
        system_message = {
            "role": "system",
            "content": (
                "You are an expert software engineer tasked with generating BM25 search queries to find "
                "relevant files in a codebase for fixing a given issue.\n\n"
                "Your goal is to create a search query that will rank the most relevant files as highly "
                "as possible using BM25 keyword matching. BM25 works by matching keywords.\n"
                "Generate a focused BM25 search query designed to rank the gold standard files "
                "as highly as possible.\n\n"
                "Format your response as:\n"
                "<thinking>[Your reasoning and analysis]</thinking>\n\n"
                "<query>your BM25 search query here</query>"
            )
        }
        
        user_message = {
            "role": "user", 
            "content": (
                f"Repository structure (grouped by directory):\n{files_text}\n\n"
                f"Issue to analyze and create a BM25 search query for:\n\n"
                f"{task.problem_statement}\n\n"
                f"Please analyze this issue and provide a BM25 search query optimized to rank the most relevant files highly."
            )
        }
        
        messages = [system_message, user_message]
        
        # Apply token limit if we have a tokenizer
        prompt_token_ids = None
        if hasattr(agent, 'llm') and hasattr(agent.llm, 'tokenizer') and agent.llm.tokenizer:
            prompt_token_ids = agent.llm.tokenizer.apply_chat_template(
                messages, add_special_tokens=True, add_generation_prompt=True
            )
            prompt_token_ids = prompt_token_ids[-self.max_prompt_length:]
            
        return Prompt(messages=messages, token_ids=prompt_token_ids)


class LocalizationAgent(Agent):
    """Simple one-step agent for generating file localization queries."""
    
    @classmethod
    def create(cls, system_prompt: str = None, llm: LLM = None, max_prompt_length: int = 8000):
        """Create a LocalizationAgent.
        
        Args:
            system_prompt: Optional system prompt override
            llm: The LLM to use
            max_prompt_length: Maximum prompt length in tokens
        """
        # Handle the llm parameter correctly for the Agent base class
        llms = llm
        if llm is not None and not isinstance(llm, dict):
            llms = {"default": llm}
            
        agent = super().create(
            llms=llms,
            nodes=[
                LocalizationNode(
                    name="localization",
                    agent_step_cls=LocalizationStep,
                    system_prompt=system_prompt if system_prompt else "",
                    max_prompt_length=max_prompt_length,
                ),
            ],
            max_iterations=1,  # Single step agent
        )
        agent.store_llm_calls = True
        if llm:
            agent.llm.load_tokenizer()
        return agent