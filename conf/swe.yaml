defaults:
  - base
  - override finetune: grpo
  - _self_

# Override the rollout policy for localization
actor:
  rollout_policy: pipelinerl.swe.rollouts.generate_unified_swe_rollout
  log_each_n_secs: 10
  llm_max_rollouts: 64
  rollout_workers: 1
  discount_factor: 1
  task_template: "{task}"
  throughput_window_size: 50
  success_threshold: 0.8  # For repair stage

# Use SWE dataset loader
dataset_loader: pipelinerl.swe.load_datasets.load_local_swe_dataset
train_dataset_names:
  - swegym
test_dataset_names:
  - swebench_lite

# Localization-specific settings
localization_agent:
  max_prompt_length: 48000
selection_agent:
  max_prompt_length: 48000
repair_agent:
  max_prompt_length: 48000

# Finetune settings optimized for localization
finetune:
  seq_length: 48000  # Shorter sequences for query generation
  gradient_accumulation_passes: 512  # Adjust based on your setup
  rl:
    temperature: ${...llm.parameters.temperature}

# LLM settings for localization
llm:
  parameters:
    max_tokens: 2000  # Short outputs for search queries
    temperature: 1.0

test_llm:
  parameters:
    max_tokens: 2000
    temperature: 0.8  # Slightly lower for more focused queries during eval

# Environment is not needed for localization
environment: null

# SWE-specific dataset parameters
dataset_loader_params:
  dataset_path: /mnt/llmd/data/swegym/ds
  test_dataset_path: /mnt/llmd/data/swebench_lite/ds

# Model path - use a coding-focused model if available
model_path: Qwen/Qwen2.5-Coder-7B-Instruct

# Evaluation settings
eval_every_n_versions: 1000

swe:
  # Stage enablement flags
  enable_localization: true
  enable_file_selection: true
  enable_repair: true
  
  # Run flags (for stages that contribute to training data)
  run_localization: true
  run_file_selection: true
  run_repair: true
  
  # Self-evaluation flags (NEW - enable generic self-eval for each stage)
  enable_localization_self_eval: true
  enable_file_selection_self_eval: true
  enable_repair_self_eval: true
  
  # Repository paths
  repo_path_train: /mnt/llmd/data/swegym/repos
  repo_path_test: /mnt/llmd/data/swebench_lite/repos
  
  # Self-evaluation and repair settings
  enable_self_eval: true
  run_self_eval: true
  
  # Abstention threshold (NEW - config-fixed threshold for all stages)
  abstention_threshold: 0.5  # Examples with self-eval score < 0.5 will be marked for abstention

swe_preprocessor_args:
  repo_path: /mnt/llmd/data/swegym/repos
  dataset_path: /mnt/llmd/data/swegym/ds
  min_token_threshold: 10
  max_token_threshold: 5000
  num_map_processes: 32
  tokenizer_model: Qwen/Qwen2.5-Coder-7B-Instruct
  force_reprocess: false
  hf_dataset_name: SWE-Gym/SWE-Gym
  hf_split_name: train

world:
  expert_llm:
    enabled: true
    model_path: openai/gpt-oss-120b
    vllm_kwargs:
      max-model-len: 65536