defaults:
    - base
    - _self_
finetune:
    seq_length: 16000
    gradient_accumulation_passes: 1024
llm:
    parameters:
        max_tokens: 1000
test_llm:
    parameters:
        max_tokens: 1000
actor:
    rollout_policy: pipelinerl.swe.rollouts.generate_swe_rollout
environment: null
dataset_loader: pipelinerl.swe.load_datasets.load_local_swe_dataset
dataset_loader_params:
    dataset_path: /mnt/llmd/data/swegym/ds
dataset_path: /mnt/llmd/data/swegym/ds
train_dataset_names:
    - swegym
test_dataset_names:
    - swegym
swe_preprocessor_args:
  dataset_path: /mnt/llmd/data/swebench_lite/ds
  repo_path: /mnt/llmd/data/swebench_lite/repos
  min_token_threshold: 10
  max_token_threshold: 10000
  num_map_processes: 8
  tokenizer_model: Qwen/Qwen2.5-Coder-7B-Instruct
  force_reprocess: false
  hf_dataset_name: princeton-nlp/SWE-bench_Lite
  hf_split_name: test
model_path: Qwen/Qwen2.5-Coder-7B-Instruct