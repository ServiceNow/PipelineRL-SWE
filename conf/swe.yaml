defaults:
  - base
  - override finetune: grpo
  - _self_

# Override the rollout policy for localization
actor:
  rollout_policy: pipelinerl.swe.rollouts.pipeline.generate_unified_swe_rollout
  log_each_n_secs: 10
  llm_max_rollouts: 64
  rollout_workers: 1
  discount_factor: 1
  task_template: "{task}"
  throughput_window_size: 50
  success_threshold: 0.8  # For repair stage

# Use SWE dataset loader
dataset_loader: pipelinerl.swe.load_datasets.load_local_swe_dataset
train_dataset_names:
  - swegym
test_dataset_names:
  - swebench_lite

# Agent-specific settings
agent:
  max_prompt_length: 48000
  selection_max_prompt_length: 48000
  repair_max_prompt_length: 48000

expert_eval:
  base_url: null
  model_name: ""
  tokenizer_name: ${expert_eval.model_name}
  parameters:
    max_tokens: 8000
    temperature: 0.0
  output_path: ${output_dir}/expert_repair_eval.jsonl
  limit: null
  connector_limit: 32
  request_timeout: 600

handoff_analysis:
  actor_glob: ""
  expert_jsonl: ${expert_eval.output_path}
  output_path: ${output_dir}/handoff_analysis.json
  threshold_start: 0.0
  threshold_stop: 1.0
  threshold_step: 0.05
  thresholds: []

# Finetune settings optimized for localization
finetune:
  seq_length: 48000  # Shorter sequences for query generation
  gradient_accumulation_passes: 512  # Adjust based on your setup
  rl:
    temperature: ${...llm.parameters.temperature}

# LLM settings for localization
llm:
  parameters:
    max_tokens: 2000  # Short outputs for search queries
    temperature: 1.0

test_llm:
  parameters:
    max_tokens: 2000
    temperature: 0.8  # Slightly lower for more focused queries during eval

# Environment is not needed for localization
environment: null

# SWE-specific dataset parameters
dataset_loader_params:
  dataset_path: /mnt/llmd/data/swegym/ds
  test_dataset_path: /mnt/llmd/data/swebench_lite/ds

# Model path - use a coding-focused model if available
model_path: Qwen/Qwen2.5-Coder-7B-Instruct

# Evaluation settings
eval_every_n_versions: 1000

swe:
  # Stage enablement flags
  enable_localization: false
  enable_file_selection: false
  enable_repair: true
  
  # Run flags (for stages that contribute to training data)
  run_localization: false
  run_file_selection: false
  run_repair: true
  
  # Self-evaluation flags (enable generic self-eval for each stage)
  enable_localization_self_eval: false
  enable_file_selection_self_eval: false
  enable_repair_self_eval: true
  
  # Repository paths
  repo_path_train: /mnt/llmd/data/swegym/repos
  repo_path_test: /mnt/llmd/data/swebench_lite/repos
  
  # Self-evaluation and repair settings
  enable_self_eval: true
  run_self_eval: true
  
  # Abstention threshold (config-fixed threshold for all stages)
  abstention_threshold: 0.5  # Examples with self-eval score < 0.5 will be marked for abstention

swe_preprocessor_args:
  repo_path: /mnt/llmd/data/swegym/repos
  dataset_path: /mnt/llmd/data/swegym/ds
  min_token_threshold: 10
  max_token_threshold: 5000
  num_map_processes: 32
  tokenizer_model: Qwen/Qwen2.5-Coder-7B-Instruct
  force_reprocess: false
  hf_dataset_name: SWE-Gym/SWE-Gym
  hf_split_name: train
