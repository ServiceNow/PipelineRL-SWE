use_flash_attention: true
attn_implementation: flash_attention_2
config_name: ${..model_path}
output_dir: ${..output_dir}/finetune
seq_length: 12000
seq_packing: true
rl:
  algo: reinforce
  divide_advantage_by_std: false
  kl_coef: 0.0
  entropy_bonus: 0.0
  reward_minus_kl_coef: 0.0
  epsilon: 4
  use_advantages: true
  relu_log_p_weights: false
  clamp_log_ratio_ref_new_value: 5
  temperature: ${...llm.parameters.temperature}
  aggregate_loss: sum
train_batch_size: 1
gradient_accumulation_passes: 1024
gradient_checkpointing: true
gradient_clipping_threshold: 0.3
# see https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d
# for the motivation to this false by default
reentrant_checkpointing: false
learning_rate: 1e-6
save_checkpoint_steps: 100
log_each_n_steps: 1
input: training_data
send_weight_updates: true
queue_size: 32
max_lag: ${..max_lag}
weight_update_interval: 1
pop_old_data: ${..pop_old_data}